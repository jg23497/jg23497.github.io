"use strict";(self.webpackChunkjgulan=self.webpackChunkjgulan||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/04/19/llms-and-an-optimistic-take-on-the-future-of-software-engineering","metadata":{"permalink":"/2025/04/19/llms-and-an-optimistic-take-on-the-future-of-software-engineering","source":"@site/blog/2025/04-19-llms-and-an-optimistic-take-on-the-future-of-software-engineering/index.mdx","title":"LLMs and an optimistic take on the future of software engineering","description":"Listen to a 10 minute AI-generated podcast version of this blog post,","date":"2025-04-19T00:00:00.000Z","tags":[{"inline":true,"label":"engineering","permalink":"/tags/engineering"},{"inline":true,"label":"ai","permalink":"/tags/ai"}],"readingTime":10.72,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"LLMs and an optimistic take on the future of software engineering","tags":["engineering","ai"]},"unlisted":false,"nextItem":{"title":"\'Command not found\' when running chruby","permalink":"/2024/02/27/chruby-error"}},"content":"import ArchimedesLever from \\"./archimedes-lever.jpg\\";\\n\\n:::note\\nListen to a 10 minute [AI-generated podcast version of this blog post](@site/static/blog/04-19-llms-and-an-optimistic-take-on-the-future-of-software-engineering/jgulan-llms-and-an-optimistic-take-on-the-future-of-software-engineering.mp3),\\ncourtesy of Google\'s [NotebookLM](https://notebooklm.google).\\n:::\\n\\n> *\u201cGive me a lever long enough and a fulcrum on which to place it, and I shall move the world.\u201d*\\n>\\n> \u2014 A quote attributed to Greek mathematician and physicist, Archimedes.\\n\\n<img align=\\"left\\" src={ArchimedesLever} alt=\\"Archimedes lifting the world\\" />\\n*Archimedes\' lever (image credit [^1])*\\n[^1]: Credit for the image to ZDF, Terra X, Gruppe 5, Susanne Utzt, Cristina Trebbi, Jens Boeck, Dieter St\xfcrmer, Fabian Wienke, \\nSebastian Martinez, which has been cropped and is licensed under the\\n[Creative Commons Attribution 4.0 International license](https://creativecommons.org/licenses/by/4.0/deed.en).\\n\\nI vividly recall my early days of programming: the dopamine hits after seeing my code successfully compile and my program run;\\nthe thing that I imagined suddenly coming to life on screen. Programming was difficult and knowledge hard-won, perhaps due to my\\nskill level at the time and the fact that documentation and guidance were not so readily available.\\n\\nThese were the days of 56k dial-up, GeoCities and Quake III Arena. Websites were individually-crafted and online communities were\\nmostly decentralised. Programming knowledge was dispersed across small and dedicated websites and communities. Then came the rapid\\nproliferation of open-source software and Stack Overflow\'s arrival, which changed the scene again, centralising programming\\ndiscussions, better disseminating knowledge and example code.\\n\\nThe world looks very different now and the landscape is shifting dramatically once more. A pivotal moment arrived in 2022 with the\\nrelease of [ChatGPT](https://chatgpt.com), the generative artificial intelligence (AI) chatbot from OpenAI, which is based on their Generative Pre-trained\\nTransformer (GPT) Large Language Model (LLM).\\n\\nWhile the initial implications might have been hazy, with each subsequent advancement, the power of LLMs and their place in our\\nfuture is becoming clearer.\\n\\n## Cognitive bias and the creative identity crisis\\n\\nThe [\\"AI effect\\"](https://en.wikipedia.org/wiki/AI_effect) describes how, once AI researchers accomplish a task previously considered indicative\\nof true AI\u2014such as defeating a human at chess\u2014the definition of genuine AI shifts, often relegating the achievement to mere\\nlogic or \\"just maths\\".\\n\\nI think part of this stems from a potential creative identity crisis and a tendency to want to hold onto skills that feel uniquely ours.\\nImagine you\'ve spent your life mastering a craft or some creative art: a studio photographer, a professional song composer, a podcaster, or a writer.\\nGenerative AI can create incredible photos, incorporating subjects (products, people, etc.) and beautiful, lifelike videos, resulting in AI-generated\\nadverts and fairly convincing movies (see [Kitsune](https://vimeo.com/1047370252)). It can create music of all different styles, write academic literature\\nreviews, poetry and stories. It can create convincing podcasts with multiple hosts who sound *real* and discuss source materials of your choice (like this\\narticle, see <a href=\\"#top\\">above</a>).\\n\\n\u201cAI slop\u201d, you say? Perhaps sometimes but the power and quality of generative AI is advancing quickly, to the point where it is a challenge\\nto keep up, and we are still working out how to make best use of it. Progress is set to continue given the incredible competition\\nbetween the key players, like OpenAI, Google, Meta AI and Anthropic. We\'re no longer rendering human hands that look like spaghetti and\\nwith too many fingers. \\n\\nA common argument is that LLMs are just statistical models, predicting the token that is most likely to follow the previous\\ntokens, over and over. Behind the glossy chat interface, the main ChatGPT API method is called Chat Completions (e.g. `POST\\nhttps://api.openai.com/v1/chat/completions`), which has connotations of predictive text completion more than real intelligence.\\nThis is true but misses the point and does not diminish the utility of LLMs.\\n\\nIn fact, the brain and LLMs work in somewhat similar ways, both by forming connections. In our brains, it is neurons communicating\\nand adapting based on learning and experience. In LLMs, it is a vast number of parameters that are adjusted during training, based\\non huge amounts of data. When we prompt an LLM, we traverse these connections, like the sparks of electrical activity that dance\\nacross our brain\'s neurons.\\n\\nAs humans, we can dream up amazing innovations and novel ideas with *true soul*, but how much of what we produce is truly original or\\ninnovative? Many songs are composed of the [same key chord progressions](https://en.wikipedia.org/wiki/List_of_chord_progressions) and\\nonly occasionally do we hear a song that truly stands out. Film scripts often repeat the same tried and tested storylines. Taking these\\n\\"building blocks\\", whether these are notes or audio frequencies that form melodies, pixels that form images, words and characters that form\\nsource code or a natural language, and knowing how to arrange them to achieve a particular goal or effect is often what matters. And that is what\\ngenerative AI seeks to do.\\n\\nWe need to remain mindful of these cognitive biases and emotional responses when approaching new developments in the\\nfield of AI, otherwise we risk overlooking or misunderstanding an incredibly useful technology.\\n\\n## LLMs and software engineers\\n\\nIt turns out LLMs can write code and they are reasonably good at it, when provided the right prompt and context. If your role is\\nto take on small and very well defined tickets and simply translate these into small code changes, then your job is ripe for\\ndisruption.\\n\\nIf we feed an LLM a prompt with the relevant context\u2014parts of or all of the codebase\u2014and clear instructions like:\\n*\u201cAdd a new \u2018birth_date\u2019 database column to the \u2018users\u2019 table of the Date type, which is nullable and has no index\u201d*, then our\\nLLM-driven agent will probably produce the correct result and will even raise a pull request.\\n\\nI notice a general trend of pessimism in online communities, particularly LinkedIn and X (Twitter), where we are told by\\ninfluencers, not engineers, that this is the end of programming and software engineering. These comments tend to attract\\na lot of attention and clicks, devolve into arguments and generate a lot of circular discussion and noise, which is probably\\ntheir intended purpose. I have a different and more optimistic take.\\n\\nMuch like the cranking of Archimedes\' imaginary lever, LLMs let us amplify human effort to produce superhuman results and for\\neveryone, and especially software engineers, this is an incredible time to be alive.\\n\\nAs software engineers, we already had great leverage. We can create and deploy an application or service and affect millions or\\nbillions of users, creating huge value for businesses and individuals and potentially changing the world. Armed with an LLM,\\nindividual software engineers and teams can now move even faster. Even better, and much as I enjoy software development, we can offload\\nsome of the drudgery and tedious work that we battle and that takes our focus away from what really matters. Maintenance work\\nlike fixing breaking API changes after a package upgrade might fall entirely to an LLM-based agent, and we can focus on the more\\nexciting, novel and meaningful work.\\n\\nThere has also never been a better time to start a side hustle. The LLM will empower software engineers to start one-person\\ncompanies and exceptional engineers will now produce even more exceptional results. Some of these companies will disrupt the\\nslow-moving incumbent companies, operating with leaner teams and tighter margins, increasing competition all around.\\n\\n## Renewing our focus\\n\\nWith one bottleneck reduced\u2014the gap between the concepts and design in our minds and the resulting source code on our\\nscreens\u2014we must shift our focus to hone our other skills and to tackle the remaining bottlenecks, which include: \\n\\n* collaboration, communication, leadership and other soft skills\\n* business and domain knowledge\\n* product and systems thinking\\n* critical thinking and problem solving\\n* problem decomposition\\n* deep technical understanding across the stack\\n* prototyping, creativity and innovation\\n* testing and test automation\\n* debugging\\n* optimisation\\n* data management and analysis\\n\\nAnd we are fortunate that LLMs can support us with these areas too.\\n\\nFor engineers, LLMs are an amplifier and multiplier. They are a [\\"sharp knife\\"](https://web.archive.org/web/20250212011320/https://signalvnoise.com/svn3/provide-sharp-knives/), perhaps the sharpest of all, and we can achieve\\nso much with them, but we must be careful not to cut our fingers. Engineering fundamentals matter now more than ever.\\n\\nSystems design and software architecture mattered before but it now matters more because poor architecture will now sooner lead to bloat,\\nduplication, complexity and poor performance. Thorough and constructive peer reviews mattered before but now matter more, especially if we\\nare moving at a faster pace and changing or adding more lines of code. An awareness of security vulnerabilities always mattered but now\\nmatters more as we move to more rapidly ship new features, relying on code that might not have been written entirely by humans. If we forget the fundamentals, \\nwe risk [vibe coding](https://en.wikipedia.org/wiki/Vibe_coding) ourselves into a corner where things start to fall apart.\\n\\n## LLMs replacing software engineers\\n\\nIn an [interview with Joe Rogan](https://youtu.be/7k1ehaE0bdU?si=BcKXwkmJC2E4dqw-&t=7697), Mark Zuckerberg of Meta recently stated:\\n\\n> *\u201cProbably in 2025, we at Meta [\u2026] are going to have an AI that can effectively be a sort of mid-level engineer that you have at your\\ncompany, that can write code.\u201d*\\n\\nAlthough apparently controversial, Zuckerberg\'s comment is similar to my point above regarding the translation of a small and well\\ndefined Jira task into code changes. He does not actually state that engineers will be replaced with agents, even if some find\\nthat idea attractive. LLM-based agents lack autonomy and agency and require direction and supervision. Let us not forget that\\nwriting a detailed prompt to produce source code is still programming of a sort, albeit at a higher level of abstraction. Churning\\nout code isn\'t enough - we still need the right code and it is up to us to decide what right is. Zuckerberg added, \\"I think [it] \\nwill augment the people working on it\\".\\n\\nIn my opinion, LLMs will not replace software engineers on the whole, but will give existing software engineers far greater\\nleverage. A much longer lever to produce greater torque at the fulcrum, if you like, greatly amplifying the initial inputs.\\n\\nThere is a caveat though. I suspect that LLMs will form a component of Artificial General Intelligence (AGI),\\nor will at least act as a stepping stone towards it, assuming AGI can indeed be created. If AGI arrives, then *all bets are off* and we enter an entirely \\nnew paradigm. In this future, no job is safe from disruption. With robotics, even physical, hands-on jobs will eventually be able \\nto be automated. There is a possible future where a robot lays bricks, rewires your home and carries in your groceries. I digress.\\n\\n### Two types of companies\\n\\nAmongst others, there will be two key types of companies. The first type will be those that see software development simply as an overhead.\\nThey are not seeking meaningful growth or to push the boundaries, and are therefore able to now achieve the same output with fewer engineers.\\nIf profit is equal to total revenue minus total expenses, then reducing engineering headcount expenses will in turn increase profit, satisfying\\nthe short-term mindset.\\n\\nOn the other hand, the second type of company will leverage LLMs to significantly multiply their engineers\' output, using the\\nproductivity boost to outmanoeuvre the competition, building better products and features, and many more of them. Combining human ingenuity\\nat scale with LLMs, these companies will run rings around the first type of company.\\n\\nFor software engineers, some turbulence lies ahead as companies begin to experiment with and adapt to LLMs, and we see the\\ntension between the two types of companies that I described above play out. Jobs will be lost, others will change and new jobs\\nwill be created. With the help of LLMs, some displaced engineers who would never have considered starting their own businesses\\nwill leave established companies to do just that.\\n\\n## Wrapping up\\n\\nOverall, I remain extremely optimistic about software engineering. Software is continuing to [\\"eat the world\\"](https://web.archive.org/web/20250410012411/https://a16z.com/why-software-is-eating-the-world) and there is no sign of\\nthis stopping. Software will one day be everywhere and in every thing and will be more critical to our lives, and we will need people\\nwho can build, understand, debug and maintain software. This is [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) in action. As software becomes\\neasier to build, we will probably want more of it and more people to develop and manage it.\\n\\nIn fact, there is already plenty of work ahead - almost all of our existing software and its interfaces are built on the fundamental\\nassumption that computers require imperative instructions, but that is no longer the case. We can talk to computers and they can\\ntalk back. We have the huge opportunity to build new systems, experiences and products using AI as a foundation. Similar\\nopportunities appeared during the smartphone revolution, which literally put powerful computers in people\'s pockets - we had to\\nrethink the assumption that computers stayed at home and rebuild accordingly. Those that adapted and took advantage of\\nthese opportunities were ultimately the winners. Let\'s get to work."},{"id":"/2024/02/27/chruby-error","metadata":{"permalink":"/2024/02/27/chruby-error","source":"@site/blog/2024/02-27-chruby-error/index.mdx","title":"\'Command not found\' when running chruby","description":"Following the Ruby community\u2019s advice to avoid using the system-installed Ruby instance for development, you","date":"2024-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"engineering","permalink":"/tags/engineering"}],"readingTime":1.77,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"\'Command not found\' when running chruby","tags":["engineering"]},"unlisted":false,"prevItem":{"title":"LLMs and an optimistic take on the future of software engineering","permalink":"/2025/04/19/llms-and-an-optimistic-take-on-the-future-of-software-engineering"},"nextItem":{"title":"Image noise reduction using DxO PhotoLab","permalink":"/2023/01/25/image-noise-reduction-using-dxo-photolab"}},"content":"Following the Ruby community\u2019s advice to avoid using the system-installed Ruby instance for development, you \\ninstall [chruby](https://github.com/postmodern/chruby) using the [official installation instructions](https://github.com/postmodern/chruby?tab=readme-ov-file#install)\\nand intend to configure your Ruby versions using [ruby-install](https://github.com/postmodern/ruby-install). After\\ninstalling chruby, you open a terminal and see the following error when running the command on your Unix-like OS\\n(Ubuntu Linux or MacOS):\\n\\n```\\n$ chruby\\nchruby: command not found\\n```\\n\\n#### What happened?\\n\\nLet\'s take a closer look at what happened. The [installation script](https://github.com/postmodern/chruby/blob/a543a35790e5528b5a67de20e78a7390f5f7606e/scripts/setup.sh#L65) automatically adds a */etc/profile.d* script at \\n*/etc/profile.d/chruby.sh*. The */etc/profile.d* scripts are themselves auto-discovered and then executed by the \\n*/etc/profile* script, which one might expect to happen upon logging in:\\n\\n```bash title=\\"Extract from Ubuntu\'s /etc/profile file\\"\\nif [ -d /etc/profile.d ]; then\\n  for i in /etc/profile.d/*.sh; do\\n    if [ -r $i ]; then\\n      . $i\\n    fi\\n  done\\n  unset i\\nfi\\n```\\n\\nHowever, */etc/profile* is invoked only for login shells. A login shell is what we use when logging in via SSH or via a TTY.\\nOur terminal instance is an interactive non-login shell and so */etc/profile* is not executed, including our new *chruby.sh* script that\\nwould have made the chruby command available to our session.\\n\\nTo make the chruby command available to an interactive shell, we must configure our shell accordingly. For Bash this involves editing the \\n*~/.bashrc* or */etc/bash.bashrc* files, which are the current user and global configuration scripts for Bash respectively:\\n\\n```bash title=\\"~/.bashrc\\"\\nsource /usr/local/share/chruby/chruby.sh\\n```\\n\\nFor ZSH on MacOS, the equivalent files would be *~/.zshrc* and */etc/zshrc*. If using a shell other than Bash or ZSH, consult the documentation\\nto ensure the correct configuration script is updated.\\n\\nCorrectly configured, we now see the following when running chruby:\\n\\n```\\n$ chruby\\n   ruby-3.3.0\\n```\\n\\n#### What next?\\n\\nAlthough the convention for Unix-like application installers is *not* to touch the user\'s data, the Google Cloud SDK installer asks users\\nwhether it should update their *~/.bashrc* file as part of the installation process:\\n\\n> Modify profile to update your $PATH and enable bash completion? (Y/n)?\\n\\nThe chruby installer could help users by offering to do the same, even if only handling the most popular shells like Bash or ZSH."},{"id":"/2023/01/25/image-noise-reduction-using-dxo-photolab","metadata":{"permalink":"/2023/01/25/image-noise-reduction-using-dxo-photolab","source":"@site/blog/2023/01-25-image-noise-reduction-using-dxo-photolab/index.mdx","title":"Image noise reduction using DxO PhotoLab","description":"Disclaimer: I am a paying user of DxO PhotoLab, but have no other association or affiliation with the company. The opinions stated here are my own and I have not been incentivised to write them.","date":"2023-01-25T00:00:00.000Z","tags":[{"inline":true,"label":"photography","permalink":"/tags/photography"}],"readingTime":2.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Image noise reduction using DxO PhotoLab","tags":["photography"]},"unlisted":false,"prevItem":{"title":"\'Command not found\' when running chruby","permalink":"/2024/02/27/chruby-error"},"nextItem":{"title":"Detecting regression commits using git bisect","permalink":"/2023/01/15/detecting-regression-commits-using-git-bisect"}},"content":"import { ReactCompareSlider, ReactCompareSliderImage } from \'react-compare-slider\';\\nimport ImageOne from \\"./noise-100pc.jpg\\";\\nimport ImageTwo from \\"./denoised-100pc.jpg\\";\\nimport ImageOneZoomed from \\"./noise-200pc.jpg\\";\\nimport ImageTwoZoomed from \\"./denoised-200pc.jpg\\";\\nimport Noise from \\"./noise.jpg\\";\\n\\n_Disclaimer: I am a paying user of DxO PhotoLab, but have no other association or affiliation with the company. The opinions stated here are my own and I have not been incentivised to write them._\\n\\nSome photographers enjoy the unobtrusive, film-like character that subtle image noise can add to an image, particularly for black-and-white photography. \\nIn my experience, however, many of us prefer clean and noise-free images for the same reason that we prefer clean and noise-free audio \u2014 noise distracts our senses and clouds the signal.\\n\\nFor images, noise can lead to the loss of detail and colour information. Luminance noise, which presents as random variations in pixel illumination, is easier to forgive, whereas chroma noise, which presents as random variations in pixel colour, is an unsightly distraction.\\n\\n<img align=\\"left\\" className=\\"with-margin\\" src={Noise} alt=\\"A noisy image\\" />\\n\\nRecent advancements in image sensor technology mean that noise is less of an issue than it was say a decade ago, but image noise will still be noticeable when shooting at higher ISO values and can be particularly problematic for smaller image sensors that gather smaller quantities of light such as 1-inch, Micro Four Thirds and APS-C sensors.\\n\\nUntil recently, noise reduction algorithms have been fairly rudimentary, often resulting in a noticeable loss of detail as the image details are \'smoothed out\' to hide the noise.\\nThis changed with the release of [DxO PhotoLab](https://www.dxo.com/dxo-photolab/)\'s DeepPRIME noise reduction algorithm. The exact inner workings of DxO\'s proprietary algorithm have not been disclosed by the company, however we know that\\nPhotoLab employs Artificial Intelligence in the form of a neural network that has been trained to perform the [demosaicing](https://en.wikipedia.org/wiki/Demosaicing) and denoising processes.\\n\\nIn any case, the results speak for themselves. Below is a 100% crop of a raw image taken of a Christmas elf figure in low-light conditions at ISO 1250 using my Olympus OM-D E-M1 Mark II, paired with the Olympus 25mm f1.2 PRO lens.\\nIf it is not already clear, the left side is the image processed without noise reduction and the right side is the image processed with PhotoLab\'s DeepPRIME noise reduction (v5.5).\\n\\n<ReactCompareSlider\\n  itemOne={<ReactCompareSliderImage src={ImageOne} alt=\\"Before noise reduction (100% crop)\\" />}\\n  itemTwo={<ReactCompareSliderImage src={ImageTwo} alt=\\"After noise reduction (100% crop)\\" />}\\n  className=\\"with-margin\\"\\n/>\\n\\nA further enlargement of the same crop follows:\\n\\n<ReactCompareSlider\\n  itemOne={<ReactCompareSliderImage src={ImageOneZoomed} alt=\\"Before noise reduction (200% crop)\\" />}\\n  itemTwo={<ReactCompareSliderImage src={ImageTwoZoomed} alt=\\"After noise reduction (200% crop)\\" />}\\n  className=\\"with-margin\\"\\n/>\\n\\n\\nThe reduction in noise is significant, all without any obvious loss of image detail. DeepPRIME noise reduction has been a game-changer for my Micro Four Thirds photography and I look forward to seeing what further enhancements\\nthe team at DxO Labs can make."},{"id":"/2023/01/15/detecting-regression-commits-using-git-bisect","metadata":{"permalink":"/2023/01/15/detecting-regression-commits-using-git-bisect","source":"@site/blog/2023/01-15-detecting-regression-commits-using-git-bisect/index.mdx","title":"Detecting regression commits using git bisect","description":"Locating the commit that introduced a regression, or some undesirable change, in a codebase can be difficult. Locating the same in a frequently-changing codebase with a large number of contributors is harder again.","date":"2023-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"engineering","permalink":"/tags/engineering"}],"readingTime":1.47,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Detecting regression commits using git bisect","tags":["engineering"]},"unlisted":false,"prevItem":{"title":"Image noise reduction using DxO PhotoLab","permalink":"/2023/01/25/image-noise-reduction-using-dxo-photolab"}},"content":"import VisualisationSvg from \\"./git-bisect.svg\\";\\nimport styles from \'./index.module.css\';\\n\\nLocating the commit that introduced a regression, or some undesirable change, in a codebase can be difficult. Locating the same in a frequently-changing codebase with a large number of contributors is harder again.\\n\\nFortunately [`git bisect`](https://git-scm.com/docs/git-bisect) provides a helpful tool for doing precisely this. Rather than performing a linear search of the commits, `git bisect` uses a clever binary search algorithm to locate the offending commit far more efficiently.\\n\\n<div class=\\"center with-margin\\">\\n  <VisualisationSvg className={styles.bisect} />\\n</div>\\n\\nHaving identified a commit from the past where the regression did not exist (e.g. `git checkout` a commit from say a month ago), my typical usage of `git bisect` is along the lines of:\\n\\n1. Check out the branch containing the regression and run `git bisect start`.\\n2. Label the good and bad commits: `git bisect good <commit hash>` and `git bisect bad <commit hash>`. The current commit is taken if the commit hash argument is omitted.\\n3. Bisecting commences \u2014 a candidate commit is automatically checked out and the number of revisions left to test and number of remaining steps are printed.  You can run `git bisect reset` to abort at any stage.\\n4. Check whether the regression still exists, whether this involves running an automated test or taking manual replication steps. If the candidate commit contains the regression, run `git bisect bad`, otherwise run `git bisect good`. If any search area remains, another candidate commit is automatically checked out.\\n5. Repeat #4 until the bisecting process concludes and the bad commit\'s hash is listed.\\n\\nGiven `git bisect`\'s binary search technique, the number of steps required will only increase logarithmically as the number of commits increases \u2014 this is a time-saving tool that I turn to again and again."}]}}')}}]);